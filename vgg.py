# -*- coding: utf-8 -*-
"""VGG

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFX-860E2dSLlV3_ZS48ZIe0hgD3Zt9t
"""

import os
import time
import shutil
import itertools

# import data handling tools
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras import regularizers
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.utils import to_categorical
# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

print ('modules loaded')



from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

skin_df =pd.read_csv('/content/drive/MyDrive/hmnist_32_32_RGB.csv')
skin_df.head()

Label = skin_df["label"]
Data = skin_df.drop(columns=["label"])

skin_df["label"].value_counts()

from imblearn.over_sampling import RandomOverSampler

# Assuming Data is a DataFrame
oversample = RandomOverSampler()
Data_array, Label = oversample.fit_resample(Data.to_numpy().reshape(-1, 32*32*3), Label)

# Reshape the array back to the original shape
Data = Data_array.reshape(-1, 32, 32, 3)
print('Shape of Data:', Data.shape)

Label = np.array(Label)
Label

classes = {
    4: ('nv', 'melanocytic nevi', 0),  # 0 for benign
    6: ('mel', 'melanoma', 1),         # 1 for malignant
    2: ('bkl', 'benign keratosis-like lesions', 0),
    1: ('bcc', 'basal cell carcinoma', 1),
    5: ('vasc', 'pyogenic granulomas and hemorrhage', 0),
    0: ('akiec', 'Actinic keratoses and intraepithelial carcinomae', 1),
    3: ('df', 'dermatofibroma', 0)
}

from sklearn.model_selection import train_test_split

X_train , X_test , y_train , y_test = train_test_split(Data , Label , test_size = 0.25 , random_state = 49)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

datagen = ImageDataGenerator(rescale=(1./255)
                             ,rotation_range=10
                             ,zoom_range = 0.1
                             ,width_shift_range=0.1
                             ,height_shift_range=0.1)

testgen = ImageDataGenerator(rescale=(1./255))

from keras.callbacks import ReduceLROnPlateau

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy'
                                            , patience = 2
                                            , verbose=1
                                            ,factor=0.5
                                            , min_lr=0.00001)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense, Dropout, GlobalMaxPooling2D, BatchNormalization
from tensorflow.keras.regularizers import l2

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

for layer in base_model.layers:
    print(layer.name)
    layer.trainable = False

print(len(base_model.layers))

last_layer =base_model.get_layer('block5_pool')
print('last layer output shape:', last_layer.output_shape)
last_output = last_layer.output

from tensorflow.keras import layers
# Flatten the output layer to 1 dimension
x = layers.GlobalMaxPooling2D()(last_output)
# Add a fully connected layer with 512 hidden units and ReLU activation
x = layers.Dense(512, activation='relu')(x)
# Add a dropout rate of 0.5
x = layers.Dropout(0.5)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(7, activation='softmax')(x)
model = Model(base_model.input, x)
optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7,amsgrad=True)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model.summary()

model.summary()

train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,
                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')

train_datagen.fit(X_train)

val_datagen = ImageDataGenerator()
val_datagen.fit(X_test)

history = model.fit(X_train ,
                    y_train ,
                    epochs=25 ,
                    batch_size=128,
                    validation_data=(X_test , y_test) ,
                    callbacks=[learning_rate_reduction]
)

model.save("vgg_model.keras")

from tensorflow.keras.models import load_model

# Load the pre-trained model
cnn_model = load_model('vgg_model_1.keras')

import joblib

joblib.dump(cnn_model, 'vgg_skin.joblib')

from sklearn.ensemble import RandomForestClassifier

loaded_cnn_model = joblib.load('vgg_skin.joblib')

X_train_features = loaded_cnn_model.predict(X_train)
X_test_features = loaded_cnn_model.predict(X_test)

X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)
X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)
rf_classifier.fit(X_train_features_flat, y_train)

y_pred = rf_classifier.predict(X_test_features_flat)

def plot_training(hist):
    tr_acc = hist.history['accuracy']
    tr_loss = hist.history['loss']
    val_acc = hist.history['val_accuracy']
    val_loss = hist.history['val_loss']
    index_loss = np.argmin(val_loss)
    val_lowest = val_loss[index_loss]
    index_acc = np.argmax(val_acc)
    acc_highest = val_acc[index_acc]

    plt.figure(figsize= (20, 8))
    plt.style.use('fivethirtyeight')
    Epochs = [i+1 for i in range(len(tr_acc))]
    loss_label = f'best epoch= {str(index_loss + 1)}'
    acc_label = f'best epoch= {str(index_acc + 1)}'

    plt.subplot(1, 2, 1)
    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout
    plt.show()

plot_training(history)

train_score = cnn_model.evaluate(X_train, y_train, verbose= 1)
test_score = cnn_model.evaluate(X_test, y_test, verbose= 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

y_true = np.array(y_test)

y_pred = np.argmax(y_pred , axis=1)
y_true = np.argmax(y_true , axis=1)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)



classes_labels = []
for key in classes.keys():
    classes_labels.append(key)

print(classes_labels)

cm = cm = confusion_matrix(y_true, y_pred, labels=classes_labels)

plt.figure(figsize= (10, 10))
plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation= 45)
plt.yticks(tick_marks, classes)


thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')

plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

plt.show()

class_names = [classes[i][0] for i in range(len(classes))]

num_images_to_visualize = 100
random_indices = np.random.choice(len(X_test), num_images_to_visualize, replace=False)

correct_predictions = 0

for idx in random_indices:
    img = X_test[idx]
    true_label, true_danger = classes[y_true[idx]][0], classes[y_true[idx]][2]
    pred_label, pred_danger = classes[y_pred[idx]][0], classes[y_pred[idx]][2]

    plt.imshow(img)
    plt.title(f"True: {true_label} ({'Benign' if true_danger == 0 else 'Malignant'})\n"
              f"Predicted: {pred_label} ({'Benign' if pred_danger == 0 else 'Malignant'})")
    plt.show()

    # Check if the prediction is correct
    if true_label == pred_label:
        correct_predictions += 1

accuracy = correct_predictions / num_images_to_visualize
print(f"Accuracy on visualized images: {accuracy * 100:.2f}%")
print(f"Correct Predictions: {correct_predictions}")

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Assuming X_train_features and X_test_features are your extracted features
# and y_train, y_test are your corresponding labels

# Initialize the Decision Tree classifier
tree_classifier = DecisionTreeClassifier()

# Train the classifier
tree_classifier.fit(X_train_features, y_train)

# Make predictions
y_pred_tree = tree_classifier.predict(X_test_features)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_tree)
print("Classification Report:\n", report)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Assuming X_train_features and X_test_features are your extracted features
# and y_train, y_test are your corresponding labels

# Initialize the Decision Tree classifier
tree_classifier = DecisionTreeClassifier()

# Train the classifier
tree_classifier.fit(X_train_features, y_train)

# Make predictions
y_pred_tree = tree_classifier.predict(X_test_features)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize the KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Train the classifier
knn_classifier.fit(X_train_features, y_train)

# Make predictions
y_pred_knn = knn_classifier.predict(X_test_features)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_knn)
print("Classification Report:\n", report)

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split

# Assuming X_train_features and X_test_features are your extracted features
# and y_train, y_test are your corresponding multi-labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_train_features, y_train, test_size=0.2, random_state=42)

# Initialize the base classifier
base_classifier = ExtraTreesClassifier(n_estimators=100, random_state=0)

# Initialize the MultiOutputClassifier
multi_output_classifier = MultiOutputClassifier(base_classifier)

# Train the classifier
multi_output_classifier.fit(X_train, y_train)

# Make predictions
y_pred_multi_output = multi_output_classifier.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_multi_output)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_multi_output)
print("Classification Report:\n", report)