# -*- coding: utf-8 -*-
"""vgg_best_ensemble

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12-lGW2CX8U0gKe7-_L6InFex2FiZ-xFA
"""

import os
import time
import shutil
import itertools

# import data handling tools
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras import regularizers
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.utils import to_categorical
# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

print ('modules loaded')

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

skin_df =pd.read_csv('/content/drive/MyDrive/hmnist_32_32_RGB.csv')
skin_df.head()

Label = skin_df["label"]
Data = skin_df.drop(columns=["label"])

skin_df["label"].value_counts()

from imblearn.over_sampling import RandomOverSampler

# Assuming Data is a DataFrame
oversample = RandomOverSampler()
Data_array, Label = oversample.fit_resample(Data.to_numpy().reshape(-1, 32*32*3), Label)

# Reshape the array back to the original shape
Data = Data_array.reshape(-1, 32, 32, 3)
print('Shape of Data:', Data.shape)

Label = np.array(Label)
Label

classes = {
    4: ('nv', 'melanocytic nevi', 0),  # 0 for benign
    6: ('mel', 'melanoma', 1),         # 1 for malignant
    2: ('bkl', 'benign keratosis-like lesions', 0),
    1: ('bcc', 'basal cell carcinoma', 1),
    5: ('vasc', 'pyogenic granulomas and hemorrhage', 0),
    0: ('akiec', 'Actinic keratoses and intraepithelial carcinomae', 1),
    3: ('df', 'dermatofibroma', 0)
}

from sklearn.model_selection import train_test_split

X_train , X_test , y_train , y_test = train_test_split(Data , Label , test_size = 0.25 , random_state = 49)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

from keras import Model, Input
input_shape = X_test[0,:,:,:].shape
model_input = Input(shape=input_shape)

model_input

model = keras.models.Sequential()

# Create Model Structure
model.add(keras.layers.Input(shape=[32, 32, 3]))
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal'))
model.add(keras.layers.MaxPooling2D())

model.add(keras.layers.Flatten())
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=256, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=128, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=64, activation='relu', kernel_initializer='he_normal'))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=32, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.L1L2()))
model.add(keras.layers.BatchNormalization())

model.add(keras.layers.Dense(units=7, activation='softmax', kernel_initializer='glorot_uniform', name='classifier'))
model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

model.summary()

model.load_weights('best_model_64.keras')

model.summary()

tf.keras.utils.plot_model(model, show_shapes = True, show_dtype = True, show_layer_names = True, rankdir="TB", expand_nested = True, dpi = 100 ,to_file='model_32_.png')

!pip install visualkeras
import visualkeras

from PIL import ImageFont
font = ImageFont.load_default()
visualkeras.layered_view(model, legend=True, font=font,to_file='output_32.png')

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))

for layer in base_model.layers:
    print(layer.name)
    layer.trainable = False

print(len(base_model.layers))

last_layer =base_model.get_layer('block5_pool')
print('last layer output shape:', last_layer.output_shape)
last_output = last_layer.output

from tensorflow.keras import layers
# Flatten the output layer to 1 dimension
x = layers.GlobalMaxPooling2D()(last_output)
# Add a fully connected layer with 512 hidden units and ReLU activation
x = layers.Dense(512, activation='relu')(x)
# Add a dropout rate of 0.5
x = layers.Dropout(0.5)(x)
# Add a final sigmoid layer for classification
x = layers.Dense(7, activation='softmax')(x)

# Configure and compile the model

model1 = Model(base_model.input, x)
optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7,amsgrad=True)
model1.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model1.load_weights("vgg_model_1.keras")

model1.summary()

tf.keras.utils.plot_model(model1, show_shapes = True, show_dtype = True, show_layer_names = True, rankdir="TB", expand_nested = True, dpi = 100 ,to_file='model_vgg_modified.png')

from PIL import ImageFont
font = ImageFont.load_default()
visualkeras.layered_view(model1, legend=True, font=font,to_file='output_vgg.png')

from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import Average
from tensorflow.keras.layers import Input

# Load your pre-trained models
model_path = '/content/best_model_64.keras'  # Replace with the actual path to your model file
model1_path = '/content/vgg_model_1.keras'  # Replace with the actual path to your model1 file



model = load_model(model_path)
model1 = load_model(model1_path)

# Assuming both models have the same input shape
model_input = model.layers[0].input  # Input layer of the first model

# Ensemble function
def ensemble(models, model_input):
    outputs = [model(model_input) for model in models]
    y = Average()(outputs)
    model = Model(model_input, y, name='ensemble')
    return model

# Create the ensemble model
ensemble_model = ensemble([model, model1], model_input)

# Compile the ensemble model
ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Display the summary of the ensemble model
ensemble_model.summary()

tf.keras.utils.plot_model(ensemble_model, show_shapes = True, show_dtype = True, show_layer_names = True, rankdir="TB", expand_nested = True, dpi = 100 ,to_file='model_ensemble_new.png')

from PIL import ImageFont
font = ImageFont.load_default()
visualkeras.layered_view(ensemble_model, legend=True, font=font,to_file='output_ensemble_new.png')

ensemble_model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

from keras.callbacks import ReduceLROnPlateau

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy'
                                            , patience = 2
                                            , verbose=1
                                            ,factor=0.5
                                            , min_lr=0.00001)

print(ensemble_model.input_shape)

history = ensemble_model.fit(X_train ,
                    y_train ,
                    epochs=25 ,
                    batch_size=128,
                    validation_data=(X_test , y_test) ,
                    callbacks=[learning_rate_reduction]
)

model.save("ensemble_model_vgg.keras")

from tensorflow.keras.models import load_model

# Load the pre-trained model
ese_model = load_model('ensemble_model_vgg.keras')

import joblib

joblib.dump(ese_model, 'ese_model_skin.joblib')

from sklearn.ensemble import RandomForestClassifier

loaded_cnn_model = joblib.load('ese_model_skin.joblib')

X_train_features = loaded_cnn_model.predict(X_train)
X_test_features = loaded_cnn_model.predict(X_test)

X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)
X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)
rf_classifier.fit(X_train_features_flat, y_train)

y_pred = rf_classifier.predict(X_test_features_flat)
y_pred

import matplotlib.pyplot as plt

# Define the training history dictionary
history = {
    'loss': [0.1487, 0.1307, 0.1224, 0.1140, 0.1046, 0.0977, 0.0955, 0.0909, 0.0828, 0.0780, 0.0753, 0.0744, 0.0729, 0.0733, 0.0724, 0.0718, 0.0710, 0.0718, 0.0705, 0.0707, 0.0695, 0.0710, 0.0692, 0.0696, 0.0682],
    'val_loss': [0.1302, 0.1239, 0.1257, 0.1147, 0.1046, 0.0903, 0.1006, 0.0967, 0.0869, 0.0828, 0.0801, 0.0808, 0.0810, 0.0806, 0.0806, 0.0803, 0.0805, 0.0790, 0.0784, 0.0791, 0.0797, 0.0782, 0.0775, 0.0774, 0.0765],
    'accuracy': [0.9955, 0.9951, 0.9956, 0.9951, 0.9962, 0.9969, 0.9963, 0.9962, 0.9981, 0.9984, 0.9986, 0.9990, 0.9989, 0.9988, 0.9988, 0.9986, 0.9986, 0.9986, 0.9989, 0.9987, 0.9990, 0.9990, 0.9988, 0.9988, 0.9988],
    'val_accuracy': [0.9807, 0.9817, 0.9791, 0.9824, 0.9832, 0.9898, 0.9840, 0.9813, 0.9859, 0.9872, 0.9870, 0.9864, 0.9861, 0.9862, 0.9865, 0.9864, 0.9863, 0.9870, 0.9871, 0.9869, 0.9866, 0.9870, 0.9875, 0.9872, 0.9876]
}

# Extract relevant metrics from the history dictionary
tr_loss = history['loss']
val_loss = history['val_loss']
tr_acc = history['accuracy']
val_acc = history['val_accuracy']
epochs = range(1, len(tr_loss) + 1)

# Plotting
plt.figure(figsize=(15, 6))

# Plotting Training and Validation Loss
plt.subplot(1, 2, 1)
plt.plot(epochs, tr_loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'g', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plotting Training and Validation Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, tr_acc, 'r', label='Training Accuracy')
plt.plot(epochs, val_acc, 'g', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

train_score = ese_model.evaluate(X_train, y_train, verbose= 1)
test_score = ese_model.evaluate(X_test, y_test, verbose= 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

y_true = np.array(y_test)

y_pred = np.argmax(y_pred , axis=1)
y_true = np.argmax(y_true , axis=1)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score

# Assuming X_train_features and X_test_features are your extracted features
# and y_train, y_test are your corresponding labels

# Initialize the Decision Tree classifier
tree_classifier = DecisionTreeClassifier()

# Train the classifier
tree_classifier.fit(X_train_features, y_train)

# Make predictions
y_pred_tree = tree_classifier.predict(X_test_features)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_tree)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_tree)
print("Classification Report:\n", report)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize the KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)

# Train the classifier
knn_classifier.fit(X_train_features, y_train)

# Make predictions
y_pred_knn = knn_classifier.predict(X_test_features)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_knn)
print("Classification Report:\n", report)

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split

# Assuming X_train_features and X_test_features are your extracted features
# and y_train, y_test are your corresponding multi-labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_train_features, y_train, test_size=0.2, random_state=42)

# Initialize the base classifier
base_classifier = ExtraTreesClassifier(n_estimators=100, random_state=0)

# Initialize the MultiOutputClassifier
multi_output_classifier = MultiOutputClassifier(base_classifier)

# Train the classifier
multi_output_classifier.fit(X_train, y_train)

# Make predictions
y_pred_multi_output = multi_output_classifier.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred_multi_output)
print("Accuracy:", accuracy)

# Generate classification report
report = classification_report(y_test, y_pred_multi_output)
print("Classification Report:\n", report)