# -*- coding: utf-8 -*-
"""Baseline CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Urn14nzmMAD-9MrZ-Bs0suyE-tuNPQky
"""

import os
import time
import shutil
import itertools

# import data handling tools
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras import regularizers
from tensorflow.keras.metrics import categorical_crossentropy

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

print ('modules loaded')

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

skin_df =pd.read_csv('/content/drive/MyDrive/hmnist_28_28_RGB.csv')
skin_df.head()

Label = skin_df["label"]
Data = skin_df.drop(columns=["label"])

skin_df["label"].value_counts()

from imblearn.over_sampling import RandomOverSampler

# Assuming Data is a DataFrame
oversample = RandomOverSampler()
Data_array, Label = oversample.fit_resample(Data.to_numpy().reshape(-1, 28*28*3), Label)

# Reshape the array back to the original shape
Data = Data_array.reshape(-1, 28, 28, 3)
print('Shape of Data:', Data.shape)

Label = np.array(Label)
Label

classes = {
    4: ('nv', 'melanocytic nevi', 0),  # 0 for benign
    6: ('mel', 'melanoma', 1),         # 1 for malignant
    2: ('bkl', 'benign keratosis-like lesions', 0),
    1: ('bcc', 'basal cell carcinoma', 1),
    5: ('vasc', 'pyogenic granulomas and hemorrhage', 0),
    0: ('akiec', 'Actinic keratoses and intraepithelial carcinomae', 1),
    3: ('df', 'dermatofibroma', 0)
}



from sklearn.model_selection import train_test_split

X_train , X_test , y_train , y_test = train_test_split(Data , Label , test_size = 0.25 , random_state = 49)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from tensorflow.keras.utils import to_categorical

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

datagen = ImageDataGenerator(rescale=(1./255)
                             ,rotation_range=10
                             ,zoom_range = 0.1
                             ,width_shift_range=0.1
                             ,height_shift_range=0.1)

testgen = ImageDataGenerator(rescale=(1./255))

from keras.callbacks import ReduceLROnPlateau

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy'
                                            , patience = 2
                                            , verbose=1
                                            ,factor=0.5
                                            , min_lr=0.00001)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import ELU

# Our input feature map is 64x64x3: 64x64 for the image pixels, and 3 for
# the three color channels: R, G, and B
img_input = Input(shape=(28, 28, 3))

# First convolution extracts 16 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = Conv2D(16, 3, activation=LeakyReLU(alpha=0.2), padding='same')(img_input)
x = MaxPooling2D(2)(x)

# Second convolution extracts 32 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = Conv2D(32, 3, activation=LeakyReLU(alpha=0.2), padding='same')(x)
x = MaxPooling2D(2)(x)

# Third convolution extracts 64 filters that are 3x3
# Convolution is followed by max-pooling layer with a 2x2 window
x = Conv2D(64, 3, activation=LeakyReLU(alpha=0.2), padding='same')(x)
x = MaxPooling2D(2)(x)

# Flatten feature map to a 1-dim tensor
x = Flatten()(x)

# Create a fully connected layer with LeakyReLU activation and 512 hidden units
x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)

# Add a dropout rate of 0.5
x = Dropout(0.5)(x)

# Create output layer with a single node and softmax activation
output = Dense(7, activation='softmax')(x)

# Configure and compile the model
model = Model(img_input, output)

optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-9, amsgrad=False)

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

history = model.fit(X_train ,
                    y_train ,
                    epochs=25 ,
                    batch_size=128,
                    validation_data=(X_test , y_test) ,
                    callbacks=[learning_rate_reduction]
)

model.save("cnn1_model.keras")

import os

# Check the current working directory
print("Current working directory:", os.getcwd())

# List files in the current directory
print("Files in current directory:", os.listdir())

from tensorflow.keras.models import load_model

# Load the pre-trained model
cnn_model = load_model('cnn1_model.keras')

import joblib

joblib.dump(cnn_model, 'cnn_model1_skin.joblib')

from sklearn.ensemble import RandomForestClassifier

loaded_cnn_model = joblib.load('cnn_model1_skin.joblib')

X_train_features = loaded_cnn_model.predict(X_train)
X_test_features = loaded_cnn_model.predict(X_test)

X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)
X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=0)
rf_classifier.fit(X_train_features_flat, y_train)

y_pred = rf_classifier.predict(X_test_features_flat)
y_pred

def plot_training(hist):
    tr_acc = hist.history['accuracy']
    tr_loss = hist.history['loss']
    val_acc = hist.history['val_accuracy']
    val_loss = hist.history['val_loss']
    index_loss = np.argmin(val_loss)
    val_lowest = val_loss[index_loss]
    index_acc = np.argmax(val_acc)
    acc_highest = val_acc[index_acc]

    plt.figure(figsize= (20, 8))
    plt.style.use('fivethirtyeight')
    Epochs = [i+1 for i in range(len(tr_acc))]
    loss_label = f'best epoch= {str(index_loss + 1)}'
    acc_label = f'best epoch= {str(index_acc + 1)}'

    plt.subplot(1, 2, 1)
    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout
    plt.show()

plot_training(history)

train_score = model.evaluate(X_train, y_train, verbose= 1)
test_score = model.evaluate(X_test, y_test, verbose= 1)

print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

y_true = np.array(y_test)

y_pred = np.argmax(y_pred , axis=1)
y_true = np.argmax(y_true , axis=1)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

classes_labels = []
for key in classes.keys():
    classes_labels.append(key)

print(classes_labels)

cm = cm = confusion_matrix(y_true, y_pred, labels=classes_labels)

plt.figure(figsize= (10, 10))
plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation= 45)
plt.yticks(tick_marks, classes)


thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')

plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predicted Label')

plt.show()

class_names = [classes[i][0] for i in range(len(classes))]

class_names = [classes[i][0] for i in range(len(classes))]
num_images_to_visualize = 10
random_indices = np.random.choice(len(X_test), num_images_to_visualize, replace=False)

correct_predictions = 0

for idx in random_indices:
    img = X_test[idx]
    true_label, true_danger = classes[y_true[idx]][0], classes[y_true[idx]][2]
    pred_label, pred_danger = classes[y_pred[idx]][0], classes[y_pred[idx]][2]

    plt.imshow(img)
    plt.title(f"True: {true_label} ({'Benign' if true_danger == 0 else 'Malignant'})\n"
              f"Predicted: {pred_label} ({'Benign' if pred_danger == 0 else 'Malignant'})")
    plt.show()

    # Check if the prediction is correct
    if true_label == pred_label:
        correct_predictions += 1

accuracy = correct_predictions / num_images_to_visualize
print(f"Accuracy on visualized images: {accuracy * 100:.2f}%")
print(f"Correct Predictions: {correct_predictions}")